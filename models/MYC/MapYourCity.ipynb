{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Earth_Observation/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from time import time_ns\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms \n",
    "from torchvision.models import resnet152, resnet101\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "from quanto import quantize, freeze, Calibration, qint8\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio \n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.transform import from_bounds\n",
    "import os  \n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30849\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "# SEED = random.randint(0,10000)\n",
    "SEED = 42\n",
    "\n",
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed) # set python seed\n",
    "    np.random.seed(seed) # seed the global NumPy random number generator(RNG)\n",
    "    torch.manual_seed(seed) # seed the RNG for all devices(both CPU and CUDA) \n",
    "\n",
    "set_random_seed(seed=SEED)\n",
    "\n",
    "# base folder (change needed)\n",
    "base_path = \"/mnt/AI4EO-MapYourCity/v1/building-age-dataset/\" # This line has to be modified/ changed  \n",
    "train_path = base_path + \"train/data/\"\n",
    "test_path =  base_path + \"test/data/\"\n",
    "\n",
    "train_data_names = os.listdir(train_path)\n",
    "test_data_names=os.listdir(test_path)\n",
    "\n",
    "print(len(train_data_names))\n",
    "\n",
    "#make validation dataset\n",
    "n=len(train_data_names)*10//100\n",
    "train_data_names, val_data_names= torch.utils.data.random_split(train_data_names, [n, len(train_data_names) - n])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYCDataset(torch.utils.data.Dataset):    \n",
    "    \"\"\"\n",
    "    This class defines the data with all the 3 modalities   \n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs,transform=None,train=True):\n",
    "        \"\"\"\n",
    "        This function initializes the data class - constructor function   \n",
    "        :param list_IDs: the PID numbers - (i.e. the pid) \n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs \n",
    "        self.transform=transform\n",
    "        self.train=train\n",
    "        self.path=train_path if train else test_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        ID = self.list_IDs[index] \n",
    "        exists=os.path.exists(self.path + ID + \"/street.jpg\")\n",
    "        if exists:\n",
    "            X = cv2.imread(self.path + ID + '/street.jpg')\n",
    "            X = cv2.resize(X,(64,64))\n",
    "            X = np.transpose(X,[2,0,1])\n",
    "\n",
    "        \n",
    "        \n",
    "        with rasterio.open(self.path + ID + '/orthophoto.tif') as src:\n",
    "            # resample data to target shape\n",
    "            X2 = src.read(\n",
    "                out_shape=(\n",
    "                    src.count,\n",
    "                    256,\n",
    "                    256\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "\n",
    "\n",
    "            #X2 = np.transpose(X2,(1,0,2))\n",
    "\n",
    "        with rasterio.open(self.path + ID + '/s2_l2a.tif') as src:\n",
    "            # resample data to target shape\n",
    "            X3 = src.read(\n",
    "                out_shape=(\n",
    "                    src.count,\n",
    "                    128,\n",
    "                    128\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "\n",
    "            #X3 = np.transpose(X3,(1,0,2))\n",
    "        \n",
    "            \n",
    "        # X2 = rasterio.open(self.path + ID + '/orthophoto.tif').read()\n",
    "        \n",
    "        # X3 = rasterio.open(self.path + ID + '/s2_l2a.tif').read() \n",
    "        \n",
    "        if self.train:\n",
    "            y = int(open(self.path + ID + '/label.txt', \"r\").read())\n",
    "            \n",
    "        if self.train:\n",
    "            return self.transform(X,X2,X3,y) if self.transform else (X,X2,X3,y)\n",
    "        if exists and not self.train:\n",
    "            return self.transform(X,X2,X3) if self.transform else (X,X2,X3)\n",
    "        return self.transform(X2,X3) if self.transform else (X2,X3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Transform definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransform:\n",
    "    def __init__(self, transform_X1, transform_X2, transform_X3):\n",
    "        self.transform_X1 = transform_X1\n",
    "        self.transform_X2 = transform_X2\n",
    "        self.transform_X3 = transform_X3\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        num_args=len(args)\n",
    "        if num_args==4:\n",
    "            return (self.transform_X1(args[0]),\\\n",
    "                self.transform_X2(args[1]).permute(1, 0, 2),\\\n",
    "                self.transform_X3(args[2]).permute(1, 0, 2),\\\n",
    "                args[3])\n",
    "        if num_args==3:\n",
    "            return (self.transform_X1(args[0]).permute(1,0,2),\\\n",
    "                self.transform_X2(args[1]).permute(1, 0, 2),\\\n",
    "                self.transform_X3(args[2]).permute(1, 0, 2))\n",
    "        return (torch.zeros(3,64,64),\\\n",
    "            self.transform_X2(args[0]).permute(1, 0, 2),\\\n",
    "            self.transform_X3(args[1]).permute(1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_x1=transforms.Compose([\n",
    "])\n",
    "transform_x2=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15)\n",
    "])\n",
    "transform_x3=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15)\n",
    "])\n",
    "\n",
    "transform=MyTransform(transform_x1,transform_x2,transform_x3)\n",
    "\n",
    "transform_x1_test=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transform_x2_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_x3_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test=MyTransform(transform_x1_test,transform_x2_test,transform_x3_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.6 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_Block(nn.Module): \n",
    "    def __init__(self, channels, reduction=16, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // self.reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // self.reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def get_activation(activation_name):\n",
    "    if activation_name == \"relu\":\n",
    "        return nn.ReLU6(inplace=True)\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.ReLU6):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.GELU):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"leaky_relu\":\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.LeakyReLU):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"prelu\":\n",
    "        return nn.PReLU()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.PReLU):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"selu\":\n",
    "        return nn.SELU(inplace=True)\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.SELU):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.Sigmoid):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.Tanh):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"mish\":\n",
    "        return nn.Mish()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.Mish):\n",
    "        return activation_name\n",
    "    else:\n",
    "        raise ValueError(f\"activation must be one of leaky_relu, prelu, selu, gelu, sigmoid, tanh, relu. Got: {activation_name}\")\n",
    "\n",
    "def get_normalization(normalization_name, num_channels, num_groups=32, dims=2):\n",
    "    if normalization_name == \"batch\":\n",
    "        if dims == 1:\n",
    "            return nn.BatchNorm1d(num_channels)\n",
    "        elif dims == 2:\n",
    "            return nn.BatchNorm2d(num_channels)\n",
    "        elif dims == 3:\n",
    "            return nn.BatchNorm3d(num_channels)\n",
    "    elif normalization_name == \"instance\":\n",
    "        if dims == 1:\n",
    "            return nn.InstanceNorm1d(num_channels)\n",
    "        elif dims == 2:\n",
    "            return nn.InstanceNorm2d(num_channels)\n",
    "        elif dims == 3:\n",
    "            return nn.InstanceNorm3d(num_channels)\n",
    "    elif normalization_name == \"layer\":\n",
    "        return nn.LayerNorm(num_channels)\n",
    "    elif normalization_name == \"group\":\n",
    "        return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)\n",
    "    elif normalization_name == \"bcn\":\n",
    "        if dims == 1:\n",
    "            return nn.Sequential(\n",
    "                nn.BatchNorm1d(num_channels),\n",
    "                nn.GroupNorm(1, num_channels)\n",
    "            )\n",
    "        elif dims == 2:\n",
    "            return nn.Sequential(\n",
    "                nn.BatchNorm2d(num_channels),\n",
    "                nn.GroupNorm(1, num_channels)\n",
    "            )\n",
    "        elif dims == 3:\n",
    "            return nn.Sequential(\n",
    "                nn.BatchNorm3d(num_channels),\n",
    "                nn.GroupNorm(1, num_channels)\n",
    "            )    \n",
    "    elif normalization_name == \"none\":\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise ValueError(f\"normalization must be one of batch, instance, layer, group, none. Got: {normalization_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreCNNBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, *, norm=\"batch\", activation=\"relu\", padding=\"same\", residual=True):\n",
    "\n",
    "        super(CoreCNNBlock, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.squeeze = SE_Block(self.out_channels)\n",
    "\n",
    "\n",
    "\n",
    "        self.match_channels = nn.Identity()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "\n",
    "            self.match_channels = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n",
    "\n",
    "                get_normalization(norm, out_channels),\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.in_channels, self.out_channels, 1, padding=0)\n",
    "\n",
    "        self.norm1 = get_normalization(norm, self.out_channels)\n",
    "\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(self.out_channels, self.out_channels, 3, padding=self.padding, groups=self.out_channels)\n",
    "\n",
    "        self.norm2 = get_normalization(norm, self.out_channels)\n",
    "\n",
    "        \n",
    "\n",
    "        self.conv3 = nn.Conv2d(self.out_channels, self.out_channels, 3, padding=self.padding, groups=1)\n",
    "\n",
    "        self.norm3 = get_normalization(norm, self.out_channels)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = x\n",
    "\n",
    "        x = self.activation(self.norm1(self.conv1(x)))\n",
    "\n",
    "        x = self.activation(self.norm2(self.conv2(x)))\n",
    "\n",
    "        x = self.norm3(self.conv3(x))\n",
    "\n",
    "        x = x * self.squeeze(x)\n",
    "\n",
    "        if self.residual:\n",
    "\n",
    "            x = x + self.match_channels(identity)\n",
    "\n",
    "        x = self.activation(x) \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CoreEncoderBlock(nn.Module): \n",
    "\n",
    "    def __init__(self, depth, in_channels, out_channels, norm=\"batch\", activation=\"relu\", padding=\"same\"):\n",
    "\n",
    "        super(CoreEncoderBlock, self).__init__() \n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.blocks = []\n",
    "\n",
    "        for i in range(self.depth): \n",
    "\n",
    "            _in_channels = self.in_channels if i == 0 else self.out_channels\n",
    "\n",
    "            block = CoreCNNBlock(_in_channels, self.out_channels, norm=self.norm, activation=self.activation, padding=self.padding)\n",
    "\n",
    "\n",
    "\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for i in range(self.depth):\n",
    "\n",
    "            x = self.blocks[i](x)\n",
    "\n",
    "        before_downsample = x\n",
    "\n",
    "        x = self.downsample(x)\n",
    "\n",
    "        return x, before_downsample\n",
    "\n",
    "\n",
    "\n",
    "class CoreAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "\n",
    "        lower_channels,\n",
    "\n",
    "        higher_channels, *,\n",
    "\n",
    "        norm=\"batch\",\n",
    "\n",
    "        activation=\"relu\",\n",
    "\n",
    "        padding=\"same\",\n",
    "\n",
    "    ):\n",
    "\n",
    "        super(CoreAttentionBlock, self).__init__()\n",
    "\n",
    "        self.lower_channels = lower_channels\n",
    "\n",
    "        self.higher_channels = higher_channels\n",
    "\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.expansion = 4\n",
    "\n",
    "        self.reduction = 4\n",
    "\n",
    "        if self.lower_channels != self.higher_channels:\n",
    "\n",
    "            self.match = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(self.higher_channels, self.lower_channels, kernel_size=1, padding=0, bias=False),\n",
    "\n",
    "                get_normalization(self.norm, self.lower_channels),\n",
    "\n",
    "            )\n",
    "\n",
    "        self.compress = nn.Conv2d(self.lower_channels, 1, kernel_size=1, padding=0)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.attn_c_pool = nn.AdaptiveAvgPool2d(self.reduction)\n",
    "\n",
    "        self.attn_c_reduction = nn.Linear(self.lower_channels * (self.reduction ** 2), self.lower_channels * self.expansion)\n",
    "\n",
    "        self.attn_c_extention = nn.Linear(self.lower_channels * self.expansion, self.lower_channels)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "\n",
    "        if x.size(1) != skip.size(1):\n",
    "\n",
    "            x = self.match(x)\n",
    "\n",
    "        x = x + skip\n",
    "\n",
    "        x = self.activation(x)\n",
    "\n",
    "        attn_spatial = self.compress(x)\n",
    "\n",
    "        attn_spatial = self.sigmoid(attn_spatial)\n",
    "\n",
    "        attn_channel = self.attn_c_pool(x)\n",
    "\n",
    "        attn_channel = attn_channel.reshape(attn_channel.size(0), -1)\n",
    "\n",
    "        attn_channel = self.attn_c_reduction(attn_channel)\n",
    "\n",
    "        attn_channel = self.activation(attn_channel)\n",
    "\n",
    "        attn_channel = self.attn_c_extention(attn_channel)\n",
    "\n",
    "        attn_channel = attn_channel.reshape(x.size(0), x.size(1), 1, 1)\n",
    "\n",
    "        attn_channel = self.sigmoid(attn_channel)\n",
    "\n",
    "        return attn_spatial, attn_channel\n",
    "\n",
    "\n",
    "\n",
    "class CoreDecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, in_channels, out_channels, *, norm=\"batch\", activation=\"relu\", padding=\"same\"):\n",
    "\n",
    "        super(CoreDecoderBlock, self).__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.activation_blocks = activation\n",
    "\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "        self.match_channels = CoreCNNBlock(self.in_channels * 2, self.out_channels, norm=self.norm, activation=self.activation_blocks, padding=self.padding)\n",
    "\n",
    "        self.attention = CoreAttentionBlock(self.in_channels, self.in_channels, norm=self.norm, activation=self.activation_blocks, padding=self.padding)\n",
    "\n",
    "        self.blocks = []\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "\n",
    "            block = CoreCNNBlock(self.out_channels, self.out_channels, norm=self.norm, activation=self.activation_blocks, padding=self.padding)\n",
    "\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x, skip):\n",
    "\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        attn_s, attn_c = self.attention(x, skip)\n",
    "\n",
    "        x = torch.cat([x, (skip * attn_s) + (skip + attn_c)], dim=1)\n",
    "\n",
    "        x = self.match_channels(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "\n",
    "            x = self.blocks[i](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CoreUnet(nn.Module):  \n",
    "\n",
    "    def __init__(self, *,\n",
    "\n",
    "        input_dim=10,\n",
    "\n",
    "        output_dim=1,\n",
    "\n",
    "        depths=None,\n",
    "\n",
    "        dims=None,\n",
    "\n",
    "        activation=\"relu\",\n",
    "\n",
    "        norm=\"batch\",\n",
    "\n",
    "        padding=\"same\",\n",
    "\n",
    "    ): \n",
    "\n",
    "        super(CoreUnet, self).__init__() \n",
    "\n",
    "        self.depths = [3, 3, 9, 3] if depths is None else depths \n",
    "\n",
    "        self.dims = [96, 192, 384, 768] if dims is None else dims\n",
    "\n",
    "        #self.depths = [3, 3, 9] if depths is None else depths\n",
    "\n",
    "        #self.dims = [96, 192, 384] if dims is None else dims\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.dims = [v // 2 for v in self.dims] \n",
    "\n",
    "        assert len(self.depths) == len(self.dims), \"depths and dims must have the same length. \"   \n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "\n",
    "            CoreCNNBlock(self.input_dim, self.dims[0], norm=self.norm, activation=self.activation, padding=self.padding),\n",
    "\n",
    "        )  \n",
    "\n",
    "        self.encoder_blocks = []  \n",
    "\n",
    "        for i in range(len(self.depths)):\n",
    "\n",
    "            encoder_block = CoreEncoderBlock(\n",
    "\n",
    "                self.depths[i],\n",
    "\n",
    "                self.dims[i - 1] if i > 0 else self.dims[0],\n",
    "\n",
    "                self.dims[i],\n",
    "\n",
    "                norm=self.norm,\n",
    "\n",
    "                activation=self.activation,\n",
    "\n",
    "                padding=self.padding,\n",
    "\n",
    "            )\n",
    "\n",
    "            self.encoder_blocks.append(encoder_block)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(self.encoder_blocks)\n",
    "\n",
    "        self.decoder_blocks = [] \n",
    "\n",
    "        for i in reversed(range(len(self.encoder_blocks))):\n",
    "\n",
    "            decoder_block = CoreDecoderBlock(\n",
    "\n",
    "                self.depths[i],\n",
    "\n",
    "                self.dims[i],\n",
    "\n",
    "                self.dims[i - 1] if i > 0 else self.dims[0],\n",
    "\n",
    "                norm=self.norm,\n",
    "\n",
    "                activation=self.activation,\n",
    "\n",
    "                padding=self.padding,\n",
    "\n",
    "            )\n",
    "\n",
    "            self.decoder_blocks.append(decoder_block)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(self.decoder_blocks)\n",
    "\n",
    "        self.bridge = nn.Sequential(\n",
    "\n",
    "            CoreCNNBlock(self.dims[-1], self.dims[-1], norm=self.norm, activation=self.activation, padding=self.padding),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "\n",
    "            CoreCNNBlock(self.dims[0], self.dims[0], norm=self.norm, activation=self.activation, padding=self.padding),\n",
    "\n",
    "            nn.Conv2d(self.dims[0], self.output_dim, kernel_size=1, padding=0),\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        skip_connections = []    \n",
    "\n",
    "        x = self.stem(x)\n",
    "\n",
    "        for block in self.encoder_blocks:\n",
    "\n",
    "            x, skip = block(x)\n",
    "\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CoreEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, *,\n",
    "\n",
    "        input_dim=10,\n",
    "\n",
    "        output_dim=1,\n",
    "\n",
    "        depths=None,\n",
    "\n",
    "        dims=None,\n",
    "\n",
    "        activation=\"relu\",\n",
    "\n",
    "        norm=\"batch\",\n",
    "\n",
    "        padding=\"same\",\n",
    "\n",
    "    ):\n",
    "\n",
    "        super(CoreEncoder, self).__init__()\n",
    "\n",
    "        self.depths = [3, 3, 9, 3] if depths is None else depths\n",
    "\n",
    "        self.dims = [96, 192, 384, 768] if dims is None else dims\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        assert len(self.depths) == len(self.dims), \"depths and dims must have the same length.\"\n",
    "\n",
    "        self.stem = CoreCNNBlock(self.input_dim, self.dims[0], norm=self.norm, activation=self.activation, padding=self.padding)\n",
    "\n",
    "        self.encoder_blocks = []  \n",
    "\n",
    "        for i in range(len(self.depths)): \n",
    "\n",
    "            encoder_block = CoreEncoderBlock(\n",
    "\n",
    "                self.depths[i],\n",
    "\n",
    "                self.dims[i - 1] if i > 0 else self.dims[0],\n",
    "\n",
    "                self.dims[i],\n",
    "\n",
    "                norm=self.norm,\n",
    "\n",
    "                activation=self.activation,\n",
    "\n",
    "                padding=self.padding,\n",
    "\n",
    "            )\n",
    "\n",
    "            self.encoder_blocks.append(encoder_block)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(self.encoder_blocks)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.dims[-1], self.output_dim),\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.stem(x)\n",
    "\n",
    "        for block in self.encoder_blocks:\n",
    "\n",
    "            x, _ = block(x)\n",
    "\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ResNet152(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained):\n",
    "\n",
    "        super(ResNet152, self).__init__() \n",
    "\n",
    "        #self.model = pretrainedmodels.__dict__['resnet152'](pretrained='imagenet')                         \n",
    "\n",
    "        #self.model = torchvision.models.resnet152(pretrained=True)                          \n",
    "\n",
    "        class MyResNet18(nn.Module):\n",
    "\n",
    "            def __init__(self, resnet, resnet2):\n",
    "\n",
    "                super().__init__()\n",
    "\n",
    "                self.features = nn.Sequential(\n",
    "\n",
    "                    resnet.conv1,\n",
    "\n",
    "                    resnet.bn1,\n",
    "\n",
    "                    resnet.relu,\n",
    "\n",
    "                    resnet.maxpool,\n",
    "\n",
    "                    resnet.layer1,\n",
    "\n",
    "                    resnet.layer2,\n",
    "\n",
    "                    resnet.layer3,\n",
    "\n",
    "                    resnet.layer4\n",
    "\n",
    "                ) \n",
    "\n",
    "                self.avgpool = resnet.avgpool\n",
    "\n",
    "                self.fc = resnet.fc\n",
    "\n",
    "\n",
    "\n",
    "                self.features2 = nn.Sequential(\n",
    "\n",
    "                    resnet2.conv1,\n",
    "\n",
    "                    resnet2.bn1,\n",
    "\n",
    "                    resnet2.relu,\n",
    "\n",
    "                    resnet2.maxpool,\n",
    "\n",
    "                    resnet2.layer1,\n",
    "\n",
    "                    resnet2.layer2,\n",
    "\n",
    "                    resnet2.layer3,\n",
    "\n",
    "                    resnet2.layer4\n",
    "\n",
    "                )\n",
    "\n",
    "                self.avgpool2 = resnet2.avgpool\n",
    "\n",
    "                self.fc2 = resnet2.fc\n",
    "\n",
    "\n",
    "\n",
    "            def _forward_impl(self, x: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "                x = self.features(x)\n",
    "\n",
    "                x = self.avgpool(x)\n",
    "\n",
    "                x = torch.flatten(x, 1)\n",
    "\n",
    "                x = self.fc(x)\n",
    "\n",
    "                x2 = self.features2(x2)\n",
    "\n",
    "                x2 = self.avgpool2(x2)\n",
    "\n",
    "                x2 = torch.flatten(x2, 1)\n",
    "\n",
    "                x2 = self.fc2(x2) \n",
    "\n",
    "                return x, x2\n",
    "\n",
    "\n",
    "\n",
    "            def forward(self, x: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "                return self._forward_impl(x, x2) \n",
    "\n",
    "\n",
    "\n",
    "        model = resnet152(weights=\"DEFAULT\")\n",
    "\n",
    "        model2 = resnet152(weights=\"DEFAULT\")\n",
    "\n",
    "        self.model = MyResNet18(model, model2)\n",
    "\n",
    "        self.l0 = nn.Linear(4480, 7)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "\n",
    "        batch, _, _, _ = x1.shape        \n",
    "\n",
    "        CHANNELS = 12\n",
    "\n",
    "        model = CoreUnet(\n",
    "\n",
    "            input_dim=CHANNELS,\n",
    "\n",
    "            output_dim=1,\n",
    "\n",
    "        ).to(device)   \n",
    "\n",
    "        x2 = self.model.features2(x2)\n",
    "        x3 = model(x3) \n",
    "\n",
    "        x2 = F.adaptive_avg_pool2d(x2, 1).reshape(batch, -1) \n",
    "        x3 = F.adaptive_avg_pool2d(x3, 1).reshape(batch, -1) \n",
    "\n",
    "        if torch.any(x1 != torch.zeros_like(x1)):        \n",
    "            x1 = self.model.features(x1)\n",
    "            x1 = F.adaptive_avg_pool2d(x1, 1).reshape(batch, -1)   \n",
    "            x = torch.cat((x1, x2, x3), 1)\n",
    "        else:\n",
    "            x = torch.cat((x2,x3),1)\n",
    "\n",
    "        l0 = self.l0(x)\n",
    "\n",
    "        return l0 \n",
    "    \n",
    "\n",
    "\n",
    "class ResNet101(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained):\n",
    "\n",
    "        super(ResNet101, self).__init__() \n",
    "\n",
    "        #self.model = pretrainedmodels.__dict__['resnet152'](pretrained='imagenet')                         \n",
    "\n",
    "        #self.model = torchvision.models.resnet152(pretrained=True)                          \n",
    "\n",
    "        class MyResNet18(nn.Module):\n",
    "\n",
    "            def __init__(self, resnet, resnet2):\n",
    "\n",
    "                super().__init__()\n",
    "\n",
    "                self.features = nn.Sequential(\n",
    "\n",
    "                    resnet.conv1,\n",
    "\n",
    "                    resnet.bn1,\n",
    "\n",
    "                    resnet.relu,\n",
    "\n",
    "                    resnet.maxpool,\n",
    "\n",
    "                    resnet.layer1,\n",
    "\n",
    "                    resnet.layer2,\n",
    "\n",
    "                    resnet.layer3,\n",
    "\n",
    "                    resnet.layer4\n",
    "\n",
    "                ) \n",
    "\n",
    "                self.avgpool = resnet.avgpool\n",
    "\n",
    "                self.fc = resnet.fc\n",
    "\n",
    "\n",
    "\n",
    "                self.features2 = nn.Sequential(\n",
    "\n",
    "                    resnet2.conv1,\n",
    "\n",
    "                    resnet2.bn1,\n",
    "\n",
    "                    resnet2.relu,\n",
    "\n",
    "                    resnet2.maxpool,\n",
    "\n",
    "                    resnet2.layer1,\n",
    "\n",
    "                    resnet2.layer2,\n",
    "\n",
    "                    resnet2.layer3,\n",
    "\n",
    "                    resnet2.layer4\n",
    "\n",
    "                )\n",
    "\n",
    "                self.avgpool2 = resnet2.avgpool\n",
    "\n",
    "                self.fc2 = resnet2.fc\n",
    "\n",
    "\n",
    "\n",
    "            def _forward_impl(self, x: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "                x = self.features(x)\n",
    "\n",
    "                x = self.avgpool(x)\n",
    "\n",
    "                x = torch.flatten(x, 1)\n",
    "\n",
    "                x = self.fc(x)\n",
    "\n",
    "                x2 = self.features2(x2)\n",
    "\n",
    "                x2 = self.avgpool2(x2)\n",
    "\n",
    "                x2 = torch.flatten(x2, 1)\n",
    "\n",
    "                x2 = self.fc2(x2) \n",
    "\n",
    "                return x, x2\n",
    "\n",
    "\n",
    "\n",
    "            def forward(self, x: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "                return self._forward_impl(x, x2) \n",
    "\n",
    "\n",
    "\n",
    "        model = resnet101(weights=\"DEFAULT\")\n",
    "\n",
    "        model2 = resnet101(weights=\"DEFAULT\")\n",
    "\n",
    "        self.model = MyResNet18(model, model2)\n",
    "\n",
    "        self.l0 = nn.Linear(4480, 7)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "\n",
    "        batch, _, _, _ = x1.shape        \n",
    "\n",
    "        CHANNELS = 12\n",
    "\n",
    "        model = CoreUnet(\n",
    "\n",
    "            input_dim=CHANNELS,\n",
    "\n",
    "            output_dim=1,\n",
    "\n",
    "        ).to(device)   \n",
    "\n",
    "        x2 = self.model.features2(x2)\n",
    "        x3 = model(x3) \n",
    "\n",
    "        x2 = F.adaptive_avg_pool2d(x2, 1).reshape(batch, -1) \n",
    "        x3 = F.adaptive_avg_pool2d(x3, 1).reshape(batch, -1) \n",
    "\n",
    "        if torch.any(x1 != torch.zeros_like(x1)):        \n",
    "            x1 = self.model.features(x1)\n",
    "            x1 = F.adaptive_avg_pool2d(x1, 1).reshape(batch, -1)   \n",
    "            x = torch.cat((x1, x2, x3), 1)\n",
    "        else:\n",
    "            x = torch.cat((x2,x3),1)\n",
    "\n",
    "        l0 = self.l0(x)\n",
    "\n",
    "        return l0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dataloader instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(base_path + \"test/test-set.csv\")\n",
    "train_df = pd.read_csv(base_path + \"train/train-set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataloader instantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MYCDataset(train_data_names,transform=transform)\n",
    "val_set = MYCDataset(val_data_names,transform=transform) \n",
    "test_set  = MYCDataset(test_data_names,transform=transform_test,train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_dataloader = DataLoader(val_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_dataloader = DataLoader(test_set,batch_size=BATCH_SIZE*2,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet101(\n",
       "  (model): MyResNet18(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (7): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (8): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (9): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (10): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (11): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (12): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (13): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (14): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (15): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (16): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (17): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (18): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (19): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (20): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (21): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (22): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (features2): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (7): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (8): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (9): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (10): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (11): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (12): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (13): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (14): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (15): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (16): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (17): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (18): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (19): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (20): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (21): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (22): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc2): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       "  (l0): Linear(in_features=4480, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model152_path = \"./model152\"\n",
    "model152 = ResNet152(pretrained=True).to(device) \n",
    "model152.train()\n",
    "\n",
    "model101_path = \"./model101\"\n",
    "model101 = ResNet101(pretrained=True).to(device) \n",
    "model101.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Eval Loop Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model,dataloader):\n",
    "    start=time_ns()\n",
    "    running_tloss = 0.\n",
    "    test_acc = 0.\n",
    "    num_tcorrect = 0\n",
    "    num_tsamples = 0\n",
    "    with torch.no_grad():\n",
    "        for _, tdata in enumerate(dataloader):\n",
    "            x1,x2,x3, tlabels = tdata\n",
    "            x1=x1.to(device,dtype=torch.float32)\n",
    "            x2=x2.to(device,dtype=torch.float32)\n",
    "            x3=x3.to(device,dtype=torch.float32)  \n",
    "            toutputs = model(x1,x2,x3)\n",
    "            tloss = criterion(toutputs, tlabels.to(device))\n",
    "            running_tloss += tloss\n",
    "            _, tpredictions = toutputs.max(dim=-1)\n",
    "            num_tcorrect += (tpredictions == tlabels.to(device)).sum()\n",
    "            num_tsamples += tpredictions.size(0)\n",
    "    avg_tloss = running_tloss/len(dataloader)\n",
    "    test_acc = float(num_tcorrect)/float(num_tsamples)\n",
    "    end=time_ns()\n",
    "    return avg_tloss,test_acc, end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train Loop Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer):\n",
    "    running_loss = 0.\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Here, we use enumerate(train_dataloader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for _, data in enumerate(train_dataloader):\n",
    "        \n",
    "        # Every data instance is an input + label pair\n",
    "        x1,x2,x3,labels = data        \n",
    "        \n",
    "        x1=x1.to(device,dtype=torch.float32)\n",
    "        x2=x2.to(device,dtype=torch.float32)\n",
    "        x3=x3.to(device,dtype=torch.float32)     \n",
    "\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(x1,x2,x3)\n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        _, predictions = outputs.max(dim=1)\n",
    "        num_correct += (predictions == labels.to(device)).sum()\n",
    "        num_samples += predictions.size(0)\n",
    "\n",
    "    train_loss = running_loss/len(train_dataloader)\n",
    "    train_acc = float(num_correct)/float(num_samples)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "def train(model,path):\n",
    "        # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('./eurosat_trainer_{}'.format(timestamp))\n",
    "    epoch_number = 0\n",
    "    tr_acc = 0.0\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "    for epoch in range(25):\n",
    "        print('============= EPOCH {} ============='.format(epoch_number + 1))\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss, tr_acc = train_one_epoch(model,optimizer)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        val_acc = 0.0\n",
    "        num_vcorrect = 0\n",
    "        num_vsamples = 0\n",
    "\n",
    "        # Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_dataloader):\n",
    "                y1,y2,y3,vlabels = vdata\n",
    "                y1=y1.to(device,dtype=torch.float32)\n",
    "                y2=y2.to(device,dtype=torch.float32)\n",
    "                y3=y3.to(device,dtype=torch.float32)     \n",
    "                voutputs = model(y1,y2,y3)\n",
    "                vloss = criterion(voutputs, vlabels.to(device))\n",
    "                running_vloss += vloss\n",
    "                _, vpredictions = voutputs.max(dim=-1)\n",
    "                num_vcorrect += (vpredictions == vlabels.to(device)).sum()\n",
    "                num_vsamples += vpredictions.size(0)\n",
    "\n",
    "        avg_vloss = running_vloss / len(val_dataloader)\n",
    "        val_acc = float(num_vcorrect)/float(num_vsamples)\n",
    "        print('LOSS : train {} | valid {}'.format(round(avg_loss, 4), round(avg_vloss.item(), 4)))\n",
    "        print('ACC  : train {}% | valid {}%'.format(round(tr_acc*100, 2), round(val_acc*100),2))\n",
    "\n",
    "        # Log the running loss averaged per epoch for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                        epoch_number + 1)\n",
    "        writer.add_scalars('Training vs. Validation Accuracy',\n",
    "                        { 'Training' : tr_acc, 'Validation' : val_acc },\n",
    "                        epoch_number + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "        epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, T, soft_target_loss_weight, ce_loss_weight, device,student_path):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "\n",
    "    tr_acc = 0.0\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(25):\n",
    "        student.train() # Student to train mode\n",
    "\n",
    "        print('============= EPOCH {} ============='.format(epoch + 1))\n",
    "\n",
    "        running_loss = 0.\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        running_vloss = 0.0\n",
    "        val_acc = 0.0\n",
    "        num_vcorrect = 0\n",
    "        num_vsamples = 0\n",
    "\n",
    "        for _, data in enumerate(train_dataloader):\n",
    "            x1,x2,x3, labels = data\n",
    "            x1=x1.to(device,dtype=torch.float32)\n",
    "            x2=x2.to(device,dtype=torch.float32)\n",
    "            x3=x3.to(device,dtype=torch.float32)\n",
    "            labels=labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(x1,x2,x3)\n",
    "\n",
    "            student_logits = student(x1,x2,x3)\n",
    "\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predictions = student_logits.max(dim=-1)\n",
    "\n",
    "            num_correct += (predictions == labels.to(device)).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        avg_loss = running_loss/len(train_dataloader)\n",
    "        tr_acc = float(num_correct)/float(num_samples)\n",
    "            # Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization.\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_dataloader):\n",
    "                y1,y2,y3,vlabels = vdata\n",
    "                y1=y1.to(device,dtype=torch.float32)\n",
    "                y2=y2.to(device,dtype=torch.float32)\n",
    "                y3=y3.to(device,dtype=torch.float32)     \n",
    "                voutputs = student(y1,y2,y3)\n",
    "                vloss = criterion(voutputs, vlabels.to(device))\n",
    "                running_vloss += vloss\n",
    "                _, vpredictions = voutputs.max(dim=-1)\n",
    "                num_vcorrect += (vpredictions == vlabels.to(device)).sum()\n",
    "                num_vsamples += vpredictions.size(0)\n",
    "\n",
    "        avg_vloss = running_vloss / len(val_dataloader)\n",
    "        val_acc = float(num_vcorrect)/float(num_vsamples)\n",
    "        print('LOSS : train {} | valid {}'.format(round(avg_loss, 4), round(avg_vloss.item(), 4)))\n",
    "        print('ACC  : train {}% | valid {}%'.format(round(tr_acc*100, 2), round(val_acc*100),2))\n",
    "\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            torch.save(student.state_dict(), student_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model152, model152_path)\n",
    "# train(model101, model101_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def size(path):\n",
    "    return os.path.getsize(path) / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model,dataloader,path):\n",
    "    res_df = test_df.copy()\n",
    "    res_df[\"predicted_label\"]=np.zeros(res_df.shape[0])\n",
    "    i=0\n",
    "    start = time_ns()\n",
    "    with torch.no_grad():\n",
    "       for _,tdata in enumerate(dataloader):\n",
    "           x1,x2,x3 = tdata\n",
    "           x1=x1.to(device,dtype=torch.float32)\n",
    "           x2=x2.to(device,dtype=torch.float32)\n",
    "           x3=x3.to(device,dtype=torch.float32)  \n",
    "           toutputs = model(x1,x2,x3)\n",
    "           _,tpredictions=toutputs.max(dim=-1)\n",
    "           j = i + len(tpredictions)\n",
    "           res_df.loc[i:j-1, \"predicted_label\"] = tpredictions.cpu().numpy()\n",
    "           i = j\n",
    "    time = time_ns() - start\n",
    "    res_df[\"predicted_label\"]=res_df[\"predicted_label\"].astype(\"int8\")\n",
    "    res_df.to_csv(path)\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "baseline_model = ResNet152(pretrained=True).to(device) \n",
    "baseline_model.load_state_dict(torch.load(model152_path))\n",
    "baseline_model.to(device)\n",
    "\n",
    "\n",
    "baseline_time = predictions(baseline_model,test_dataloader ,\"baseline_submission.csv\")\n",
    "baseline_size = size(model152_path)\n",
    "baseline_parameters = count_parameters(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pruned model:\n",
      " - has 100.00% of parameters\n",
      " - is 1.00 times smaller\n",
      " - is 1.04 times faster\n"
     ]
    }
   ],
   "source": [
    "pruned_model=ResNet152(pretrained=True)\n",
    "pruned_model.load_state_dict(torch.load(model152_path))\n",
    "pruned_model.to(device)\n",
    "pruned_model.eval()\n",
    "\n",
    "pruned_path = \"./pruned_model_0_2\"\n",
    "\n",
    "parameters_to_prune = []\n",
    "for module_name, module in pruned_model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        parameters_to_prune.append((module, \"weight\"))\n",
    "\n",
    "## to send pruning 0.5\n",
    "# The pruned model:\n",
    "#  - has 100.00% of parameters\n",
    "#  - is 1.00 times smaller\n",
    "#  - is 1.03 times faster\n",
    "\n",
    "## to send pruning 0.4\n",
    "# The pruned model:\n",
    "#  - has 100.00% of parameters\n",
    "#  - is 1.00 times smaller\n",
    "#  - is 1.01 times faster\n",
    "\n",
    "## to send pruning 0.3\n",
    "# The pruned model:\n",
    "#  - has 100.00% of parameters\n",
    "#  - is 1.00 times smaller\n",
    "#  - is 1.05 times faster\n",
    "\n",
    "## to send pruning 0.2\n",
    "# The pruned model:\n",
    "#  - has 100.00% of parameters\n",
    "#  - is 1.00 times smaller\n",
    "#  - is 1.04 times faster\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2\n",
    ")\n",
    "\n",
    "for module,name in parameters_to_prune:\n",
    "    prune.remove(module,name)\n",
    "\n",
    "torch.save(pruned_model.state_dict(), pruned_path)\n",
    "\n",
    "pruned_time = predictions(pruned_model, test_dataloader,\"pruned_submission_0_2.csv\")\n",
    "pruned_size = size(pruned_path)\n",
    "pruned_parameters = count_parameters(pruned_model)\n",
    "\n",
    "print(f\"The pruned model:\") \n",
    "print(f\" - has {(pruned_parameters/baseline_parameters)*100:.2f}% of parameters\")\n",
    "print(f\" - is {baseline_size/pruned_size:.2f} times smaller\")\n",
    "print(f\" - is {baseline_time/pruned_time:.2f} times faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The small model:\n",
      " - has 74.02% of parameters\n",
      " - is 1.35 times smaller\n",
      " - is 1.10 times faster\n",
      "The student model:\n",
      " - has 74.02% of parameters\n",
      " - is 1.35 times smaller\n",
      " - is 1.11 times faster\n"
     ]
    }
   ],
   "source": [
    "teacher_model=ResNet152(pretrained=True)\n",
    "teacher_model.load_state_dict(torch.load(model152_path))\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "small_model=ResNet101(pretrained=True)\n",
    "small_model.load_state_dict(torch.load(model101_path))\n",
    "small_model.to(device)\n",
    "small_model.eval()\n",
    "\n",
    "student_path = \"./student_model\"\n",
    "student_model=ResNet101(pretrained=True)\n",
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "# train_knowledge_distillation(teacher=teacher_model, student=student_model, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device,student_path=student_path)\n",
    "student_model.load_state_dict(torch.load(student_path))\n",
    "\n",
    "small_time = predictions(small_model, test_dataloader,\"small_submission.csv\")\n",
    "small_size = size(model101_path)\n",
    "small_parameters = count_parameters(small_model)\n",
    "print(f\"The small model:\") \n",
    "print(f\" - has {(small_parameters/baseline_parameters)*100:.2f}% of parameters\")\n",
    "print(f\" - is {baseline_size/small_size:.2f} times smaller\")\n",
    "print(f\" - is {baseline_time/small_time:.2f} times faster\")\n",
    "\n",
    "student_time = predictions(student_model, test_dataloader, \"student_submission.csv\")\n",
    "student_size = size(student_path)\n",
    "student_parameters = count_parameters(student_model)\n",
    "print(f\"The student model:\") \n",
    "print(f\" - has {(student_parameters/baseline_parameters)*100:.2f}% of parameters\")\n",
    "print(f\" - is {baseline_size/student_size:.2f} times smaller\")\n",
    "print(f\" - is {baseline_time/student_time:.2f} times faster\")\n",
    "\n",
    "# The small model:\n",
    "#  - has 74.02% of parameters\n",
    "#  - is 1.35 times smaller\n",
    "#  - is 1.07 times faster\n",
    "# The student model:\n",
    "#  - has 74.02% of parameters\n",
    "#  - is 1.35 times smaller\n",
    "#  - is 1.08 times faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal(model,dataloader):\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader):\n",
    "            x1=data[0].to(device,dtype=torch.float32)\n",
    "            x2=data[1].to(device,dtype=torch.float32)\n",
    "            x3=data[2].to(device,dtype=torch.float32)\n",
    "            model(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_path = \"./quantized_model\"\n",
    "quantized_model = ResNet152(pretrained = True)\n",
    "quantized_model.load_state_dict(torch.load(model152_path))\n",
    "quantized_model.to(device)\n",
    "quantized_model.eval()\n",
    "\n",
    "quantize(quantized_model, weights=qint8, activations=qint8)\n",
    "\n",
    "with Calibration():\n",
    "    cal(quantized_model, train_dataloader)\n",
    "\n",
    "freeze(quantized_model)\n",
    "\n",
    "quantized_model.qconfig = torch.quantization.get_default_qconfig('x86')\n",
    "\n",
    "quantized_model = torch.quantization.convert(quantized_model)\n",
    "\n",
    "torch.save(quantized_model.state_dict(), quantized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quantized model:\n",
      " - has 100.00% of parameters\n",
      " - is 3.89 times smaller\n",
      " - is 1.04 times faster\n"
     ]
    }
   ],
   "source": [
    "quantized_time = predictions(quantized_model,test_dataloader, \"quantized_submission.csv\")\n",
    "quantized_size = size(quantized_path)\n",
    "quantized_parameters = count_parameters(quantized_model)\n",
    "print(f\"The quantized model:\") \n",
    "print(f\" - has {(quantized_parameters/baseline_parameters)*100:.2f}% of parameters\")\n",
    "print(f\" - is {baseline_size/quantized_size:.2f} times smaller\")\n",
    "print(f\" - is {baseline_time/quantized_time:.2f} times faster\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
