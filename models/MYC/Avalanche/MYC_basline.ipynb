{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from time import time_ns\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms \n",
    "from torchvision.models import resnet152\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio \n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.transform import from_bounds\n",
    "import os  \n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from avalanche.benchmarks import nc_benchmark, benchmark_with_validation_stream\n",
    "from avalanche.benchmarks.utils import as_classification_dataset\n",
    "from avalanche.benchmarks.datasets import default_dataset_location \n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, loss_metrics\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive, Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "# SEED = random.randint(0,10000)\n",
    "SEED = 42\n",
    "\n",
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed) # set python seed\n",
    "    np.random.seed(seed) # seed the global NumPy random number generator(RNG)\n",
    "    torch.manual_seed(seed) # seed the RNG for all devices(both CPU and CUDA) \n",
    "\n",
    "set_random_seed(seed=SEED)\n",
    "\n",
    "# base folder (change needed)\n",
    "base_path = \"/disk4/gdpalma/AI4EO-MapYourCity/v1/building-age-dataset/\" # This line has to be modified/ changed  \n",
    "train_path = base_path + \"train/data/\"\n",
    "test_path =  base_path + \"test/data/\"\n",
    "\n",
    "train_data_names = os.listdir(train_path)\n",
    "test_data_names=os.listdir(test_path)\n",
    "\n",
    "# print(len(train_data_names))\n",
    "\n",
    "#make validation dataset\n",
    "n=len(train_data_names)*10//100\n",
    "train_data_names, val_data_names= torch.utils.data.random_split(train_data_names, [n, len(train_data_names) - n])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MYCDataset(torch.utils.data.Dataset):    \n",
    "#     \"\"\"\n",
    "#     This class defines the data with all the 3 modalities   \n",
    "#     \"\"\"\n",
    "#     def __init__(self, list_IDs,transform=None,train=True):\n",
    "#         \"\"\"\n",
    "#         This function initializes the data class - constructor function   \n",
    "#         :param list_IDs: the PID numbers - (i.e. the pid) \n",
    "#         \"\"\"\n",
    "#         self.list_IDs = list_IDs \n",
    "#         self.transform=transform\n",
    "#         self.train=train\n",
    "#         self.path=train_path if train else test_path\n",
    "#         self.targets: List[int] = []\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.list_IDs)\n",
    "\n",
    "#     def __getitem__(self, index): \n",
    "#         ID = self.list_IDs[index] \n",
    "#         exists=os.path.exists(self.path + ID + \"/street.jpg\")\n",
    "#         if exists:\n",
    "#             X = cv2.imread(self.path + ID + '/street.jpg')\n",
    "#             X = cv2.resize(X,(64,64))\n",
    "#             X = np.transpose(X,[2,0,1])\n",
    "\n",
    "        \n",
    "        \n",
    "#         with rasterio.open(self.path + ID + '/orthophoto.tif') as src:\n",
    "#             # resample data to target shape\n",
    "#             X2 = src.read(\n",
    "#                 out_shape=(\n",
    "#                     src.count,\n",
    "#                     256,\n",
    "#                     256\n",
    "#                 ),\n",
    "#                 resampling=Resampling.bilinear\n",
    "#             )\n",
    "\n",
    "\n",
    "#             #X2 = np.transpose(X2,(1,0,2))\n",
    "\n",
    "#         with rasterio.open(self.path + ID + '/s2_l2a.tif') as src:\n",
    "#             # resample data to target shape\n",
    "#             X3 = src.read(\n",
    "#                 out_shape=(\n",
    "#                     src.count,\n",
    "#                     128,\n",
    "#                     128\n",
    "#                 ),\n",
    "#                 resampling=Resampling.bilinear\n",
    "#             )\n",
    "\n",
    "#             #X3 = np.transpose(X3,(1,0,2))\n",
    "        \n",
    "            \n",
    "#         # X2 = rasterio.open(self.path + ID + '/orthophoto.tif').read()\n",
    "        \n",
    "#         # X3 = rasterio.open(self.path + ID + '/s2_l2a.tif').read() \n",
    "        \n",
    "#         if self.train:\n",
    "#             y = int(open(self.path + ID + '/label.txt', \"r\").read())\n",
    "            \n",
    "#         if self.train:\n",
    "#             return self.transform(X,X2,X3),y if self.transform else (X,X2,X3),y\n",
    "#         if exists and not self.train:\n",
    "#             return self.transform(X,X2,X3) if self.transform else (X,X2,X3)\n",
    "#         return self.transform(X2,X3) if self.transform else (X2,X3),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from torch.utils.data import Dataset\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "\n",
    "class MYCDataset(Dataset):    \n",
    "    \"\"\"\n",
    "    This class defines the data with all the 3 modalities   \n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs, transform=False, train=True):\n",
    "        \"\"\"\n",
    "        This function initializes the data class - constructor function   \n",
    "        :param list_IDs: the PID numbers - (i.e. the pid) \n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs \n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.path = train_path if train else test_path\n",
    "        self.targets = []  # Initialize targets list\n",
    "        self.load_labels()  # Load labels during instantiation\n",
    "\n",
    "    def load_labels(self):\n",
    "        for ID in self.list_IDs:\n",
    "            label_path = os.path.join(self.path, ID, 'label.txt')\n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, \"r\") as f:\n",
    "                    label = int(f.read())\n",
    "                    self.targets.append(label)\n",
    "            else:\n",
    "                # If label file doesn't exist, append a placeholder\n",
    "                self.targets.append(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        ID = self.list_IDs[index] \n",
    "        exists = os.path.exists(os.path.join(self.path, ID, \"street.jpg\"))\n",
    "        if exists:\n",
    "            X = cv2.imread(os.path.join(self.path, ID, 'street.jpg'))\n",
    "            X = cv2.resize(X, (64, 64))\n",
    "            X = np.transpose(X, [2, 0, 1])\n",
    "\n",
    "        with rasterio.open(os.path.join(self.path, ID, 'orthophoto.tif')) as src:\n",
    "            X2 = src.read(\n",
    "                out_shape=(\n",
    "                    src.count,\n",
    "                    256,\n",
    "                    256\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "\n",
    "        with rasterio.open(os.path.join(self.path, ID, 's2_l2a.tif')) as src:\n",
    "            X3 = src.read(\n",
    "                out_shape=(\n",
    "                    src.count,\n",
    "                    128,\n",
    "                    128\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "\n",
    "\n",
    "        if self.train:\n",
    "            y = self.targets[index]  # Get the label from pre-loaded targets\n",
    "            #return self.transform(X, X2, X3),y if self.transform else \n",
    "            #return X, X2, X3, y\n",
    "            return (X,X2,X3),y\n",
    "        else:\n",
    "            if exists:\n",
    "                #return self.transform(X, X2, X3) if self.transform else \n",
    "                return (X, X2, X3)\n",
    "            # return self.transform(X2, X3) if self.transform else \n",
    "            return (X2, X3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Transform definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransform:\n",
    "    def __init__(self, transform_X1, transform_X2, transform_X3):\n",
    "        self.transform_X1 = transform_X1\n",
    "        self.transform_X2 = transform_X2\n",
    "        self.transform_X3 = transform_X3\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        num_args=len(args)\n",
    "        if num_args==3:\n",
    "            return self.transform_X1(args[0]).permute(1,0,2),\\\n",
    "                self.transform_X2(args[1]).permute(1, 0, 2),\\\n",
    "                self.transform_X3(args[2]).permute(1, 0, 2)\n",
    "        return torch.zeros(3,64,64),\\\n",
    "            self.transform_X2(args[0]).permute(1, 0, 2),\\\n",
    "            self.transform_X3(args[1]).permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_x1=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transform_x2=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15)\n",
    "])\n",
    "transform_x3=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15)\n",
    "])\n",
    "\n",
    "transform=MyTransform(transform_x1,transform_x2,transform_x3)\n",
    "\n",
    "transform_x1_test=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transform_x2_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_x3_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test=MyTransform(transform_x1_test,transform_x2_test,transform_x3_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.6 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_Block(nn.Module): \n",
    "    def __init__(self, channels, reduction=16, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // self.reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // self.reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def get_activation(activation_name):\n",
    "    if activation_name == \"relu\":\n",
    "        return nn.ReLU6(inplace=True)\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.ReLU6):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.GELU):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"leaky_relu\":\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.LeakyReLU):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"prelu\":\n",
    "        return nn.PReLU()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.PReLU):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"selu\":\n",
    "        return nn.SELU(inplace=True)\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.SELU):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.Sigmoid):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.Tanh):\n",
    "        return activation_name\n",
    "\n",
    "    elif activation_name == \"mish\":\n",
    "        return nn.Mish()\n",
    "    elif isinstance(activation_name, torch.nn.modules.activation.Mish):\n",
    "        return activation_name\n",
    "    else:\n",
    "        raise ValueError(f\"activation must be one of leaky_relu, prelu, selu, gelu, sigmoid, tanh, relu. Got: {activation_name}\")\n",
    "\n",
    "def get_normalization(normalization_name, num_channels, num_groups=32, dims=2):\n",
    "    if normalization_name == \"batch\":\n",
    "        if dims == 1:\n",
    "            return nn.BatchNorm1d(num_channels)\n",
    "        elif dims == 2:\n",
    "            return nn.BatchNorm2d(num_channels)\n",
    "        elif dims == 3:\n",
    "            return nn.BatchNorm3d(num_channels)\n",
    "    elif normalization_name == \"instance\":\n",
    "        if dims == 1:\n",
    "            return nn.InstanceNorm1d(num_channels)\n",
    "        elif dims == 2:\n",
    "            return nn.InstanceNorm2d(num_channels)\n",
    "        elif dims == 3:\n",
    "            return nn.InstanceNorm3d(num_channels)\n",
    "    elif normalization_name == \"layer\":\n",
    "        return nn.LayerNorm(num_channels)\n",
    "    elif normalization_name == \"group\":\n",
    "        return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)\n",
    "    elif normalization_name == \"bcn\":\n",
    "        if dims == 1:\n",
    "            return nn.Sequential(\n",
    "                nn.BatchNorm1d(num_channels),\n",
    "                nn.GroupNorm(1, num_channels)\n",
    "            )\n",
    "        elif dims == 2:\n",
    "            return nn.Sequential(\n",
    "                nn.BatchNorm2d(num_channels),\n",
    "                nn.GroupNorm(1, num_channels)\n",
    "            )\n",
    "        elif dims == 3:\n",
    "            return nn.Sequential(\n",
    "                nn.BatchNorm3d(num_channels),\n",
    "                nn.GroupNorm(1, num_channels)\n",
    "            )    \n",
    "    elif normalization_name == \"none\":\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise ValueError(f\"normalization must be one of batch, instance, layer, group, none. Got: {normalization_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreCNNBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, *, norm=\"batch\", activation=\"relu\", padding=\"same\", residual=True):\n",
    "\n",
    "        super(CoreCNNBlock, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.squeeze = SE_Block(self.out_channels)\n",
    "\n",
    "\n",
    "\n",
    "        self.match_channels = nn.Identity()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "\n",
    "            self.match_channels = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n",
    "\n",
    "                get_normalization(norm, out_channels),\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.in_channels, self.out_channels, 1, padding=0)\n",
    "\n",
    "        self.norm1 = get_normalization(norm, self.out_channels)\n",
    "\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(self.out_channels, self.out_channels, 3, padding=self.padding, groups=self.out_channels)\n",
    "\n",
    "        self.norm2 = get_normalization(norm, self.out_channels)\n",
    "\n",
    "        \n",
    "\n",
    "        self.conv3 = nn.Conv2d(self.out_channels, self.out_channels, 3, padding=self.padding, groups=1)\n",
    "\n",
    "        self.norm3 = get_normalization(norm, self.out_channels)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = x\n",
    "\n",
    "        x = self.activation(self.norm1(self.conv1(x)))\n",
    "\n",
    "        x = self.activation(self.norm2(self.conv2(x)))\n",
    "\n",
    "        x = self.norm3(self.conv3(x))\n",
    "\n",
    "        x = x * self.squeeze(x)\n",
    "\n",
    "        if self.residual:\n",
    "\n",
    "            x = x + self.match_channels(identity)\n",
    "\n",
    "        x = self.activation(x) \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CoreEncoderBlock(nn.Module): \n",
    "\n",
    "    def __init__(self, depth, in_channels, out_channels, norm=\"batch\", activation=\"relu\", padding=\"same\"):\n",
    "\n",
    "        super(CoreEncoderBlock, self).__init__() \n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.blocks = []\n",
    "\n",
    "        for i in range(self.depth): \n",
    "\n",
    "            _in_channels = self.in_channels if i == 0 else self.out_channels\n",
    "\n",
    "            block = CoreCNNBlock(_in_channels, self.out_channels, norm=self.norm, activation=self.activation, padding=self.padding)\n",
    "\n",
    "\n",
    "\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for i in range(self.depth):\n",
    "\n",
    "            x = self.blocks[i](x)\n",
    "\n",
    "        before_downsample = x\n",
    "\n",
    "        x = self.downsample(x)\n",
    "\n",
    "        return x, before_downsample\n",
    "\n",
    "\n",
    "\n",
    "class CoreAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "\n",
    "        lower_channels,\n",
    "\n",
    "        higher_channels, *,\n",
    "\n",
    "        norm=\"batch\",\n",
    "\n",
    "        activation=\"relu\",\n",
    "\n",
    "        padding=\"same\",\n",
    "\n",
    "    ):\n",
    "\n",
    "        super(CoreAttentionBlock, self).__init__()\n",
    "\n",
    "        self.lower_channels = lower_channels\n",
    "\n",
    "        self.higher_channels = higher_channels\n",
    "\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.expansion = 4\n",
    "\n",
    "        self.reduction = 4\n",
    "\n",
    "        if self.lower_channels != self.higher_channels:\n",
    "\n",
    "            self.match = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(self.higher_channels, self.lower_channels, kernel_size=1, padding=0, bias=False),\n",
    "\n",
    "                get_normalization(self.norm, self.lower_channels),\n",
    "\n",
    "            )\n",
    "\n",
    "        self.compress = nn.Conv2d(self.lower_channels, 1, kernel_size=1, padding=0)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.attn_c_pool = nn.AdaptiveAvgPool2d(self.reduction)\n",
    "\n",
    "        self.attn_c_reduction = nn.Linear(self.lower_channels * (self.reduction ** 2), self.lower_channels * self.expansion)\n",
    "\n",
    "        self.attn_c_extention = nn.Linear(self.lower_channels * self.expansion, self.lower_channels)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "\n",
    "        if x.size(1) != skip.size(1):\n",
    "\n",
    "            x = self.match(x)\n",
    "\n",
    "        x = x + skip\n",
    "\n",
    "        x = self.activation(x)\n",
    "\n",
    "        attn_spatial = self.compress(x)\n",
    "\n",
    "        attn_spatial = self.sigmoid(attn_spatial)\n",
    "\n",
    "        attn_channel = self.attn_c_pool(x)\n",
    "\n",
    "        attn_channel = attn_channel.reshape(attn_channel.size(0), -1)\n",
    "\n",
    "        attn_channel = self.attn_c_reduction(attn_channel)\n",
    "\n",
    "        attn_channel = self.activation(attn_channel)\n",
    "\n",
    "        attn_channel = self.attn_c_extention(attn_channel)\n",
    "\n",
    "        attn_channel = attn_channel.reshape(x.size(0), x.size(1), 1, 1)\n",
    "\n",
    "        attn_channel = self.sigmoid(attn_channel)\n",
    "\n",
    "        return attn_spatial, attn_channel\n",
    "\n",
    "\n",
    "\n",
    "class CoreDecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, in_channels, out_channels, *, norm=\"batch\", activation=\"relu\", padding=\"same\"):\n",
    "\n",
    "        super(CoreDecoderBlock, self).__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.activation_blocks = activation\n",
    "\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "        self.match_channels = CoreCNNBlock(self.in_channels * 2, self.out_channels, norm=self.norm, activation=self.activation_blocks, padding=self.padding)\n",
    "\n",
    "        self.attention = CoreAttentionBlock(self.in_channels, self.in_channels, norm=self.norm, activation=self.activation_blocks, padding=self.padding)\n",
    "\n",
    "        self.blocks = []\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "\n",
    "            block = CoreCNNBlock(self.out_channels, self.out_channels, norm=self.norm, activation=self.activation_blocks, padding=self.padding)\n",
    "\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x, skip):\n",
    "\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        attn_s, attn_c = self.attention(x, skip)\n",
    "\n",
    "        x = torch.cat([x, (skip * attn_s) + (skip + attn_c)], dim=1)\n",
    "\n",
    "        x = self.match_channels(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "\n",
    "            x = self.blocks[i](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CoreUnet(nn.Module):  \n",
    "\n",
    "    def __init__(self, *,\n",
    "\n",
    "        input_dim=10,\n",
    "\n",
    "        output_dim=1,\n",
    "\n",
    "        depths=None,\n",
    "\n",
    "        dims=None,\n",
    "\n",
    "        activation=\"relu\",\n",
    "\n",
    "        norm=\"batch\",\n",
    "\n",
    "        padding=\"same\",\n",
    "\n",
    "    ): \n",
    "\n",
    "        super(CoreUnet, self).__init__() \n",
    "\n",
    "        self.depths = [3, 3, 9, 3] if depths is None else depths \n",
    "\n",
    "        self.dims = [96, 192, 384, 768] if dims is None else dims\n",
    "\n",
    "        #self.depths = [3, 3, 9] if depths is None else depths\n",
    "\n",
    "        #self.dims = [96, 192, 384] if dims is None else dims\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        self.dims = [v // 2 for v in self.dims] \n",
    "\n",
    "        assert len(self.depths) == len(self.dims), \"depths and dims must have the same length. \"   \n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "\n",
    "            CoreCNNBlock(self.input_dim, self.dims[0], norm=self.norm, activation=self.activation, padding=self.padding),\n",
    "\n",
    "        )  \n",
    "\n",
    "        self.encoder_blocks = []  \n",
    "\n",
    "        for i in range(len(self.depths)):\n",
    "\n",
    "            encoder_block = CoreEncoderBlock(\n",
    "\n",
    "                self.depths[i],\n",
    "\n",
    "                self.dims[i - 1] if i > 0 else self.dims[0],\n",
    "\n",
    "                self.dims[i],\n",
    "\n",
    "                norm=self.norm,\n",
    "\n",
    "                activation=self.activation,\n",
    "\n",
    "                padding=self.padding,\n",
    "\n",
    "            )\n",
    "\n",
    "            self.encoder_blocks.append(encoder_block)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(self.encoder_blocks)\n",
    "\n",
    "        self.decoder_blocks = [] \n",
    "\n",
    "        for i in reversed(range(len(self.encoder_blocks))):\n",
    "\n",
    "            decoder_block = CoreDecoderBlock(\n",
    "\n",
    "                self.depths[i],\n",
    "\n",
    "                self.dims[i],\n",
    "\n",
    "                self.dims[i - 1] if i > 0 else self.dims[0],\n",
    "\n",
    "                norm=self.norm,\n",
    "\n",
    "                activation=self.activation,\n",
    "\n",
    "                padding=self.padding,\n",
    "\n",
    "            )\n",
    "\n",
    "            self.decoder_blocks.append(decoder_block)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(self.decoder_blocks)\n",
    "\n",
    "        self.bridge = nn.Sequential(\n",
    "\n",
    "            CoreCNNBlock(self.dims[-1], self.dims[-1], norm=self.norm, activation=self.activation, padding=self.padding),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "\n",
    "            CoreCNNBlock(self.dims[0], self.dims[0], norm=self.norm, activation=self.activation, padding=self.padding),\n",
    "\n",
    "            nn.Conv2d(self.dims[0], self.output_dim, kernel_size=1, padding=0),\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        skip_connections = []    \n",
    "\n",
    "        x = self.stem(x)\n",
    "\n",
    "        for block in self.encoder_blocks:\n",
    "\n",
    "            x, skip = block(x)\n",
    "\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CoreEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, *,\n",
    "\n",
    "        input_dim=10,\n",
    "\n",
    "        output_dim=1,\n",
    "\n",
    "        depths=None,\n",
    "\n",
    "        dims=None,\n",
    "\n",
    "        activation=\"relu\",\n",
    "\n",
    "        norm=\"batch\",\n",
    "\n",
    "        padding=\"same\",\n",
    "\n",
    "    ):\n",
    "\n",
    "        super(CoreEncoder, self).__init__()\n",
    "\n",
    "        self.depths = [3, 3, 9, 3] if depths is None else depths\n",
    "\n",
    "        self.dims = [96, 192, 384, 768] if dims is None else dims\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.norm = norm\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        assert len(self.depths) == len(self.dims), \"depths and dims must have the same length.\"\n",
    "\n",
    "        self.stem = CoreCNNBlock(self.input_dim, self.dims[0], norm=self.norm, activation=self.activation, padding=self.padding)\n",
    "\n",
    "        self.encoder_blocks = []  \n",
    "\n",
    "        for i in range(len(self.depths)): \n",
    "\n",
    "            encoder_block = CoreEncoderBlock(\n",
    "\n",
    "                self.depths[i],\n",
    "\n",
    "                self.dims[i - 1] if i > 0 else self.dims[0],\n",
    "\n",
    "                self.dims[i],\n",
    "\n",
    "                norm=self.norm,\n",
    "\n",
    "                activation=self.activation,\n",
    "\n",
    "                padding=self.padding,\n",
    "\n",
    "            )\n",
    "\n",
    "            self.encoder_blocks.append(encoder_block)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(self.encoder_blocks)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.dims[-1], self.output_dim),\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.stem(x)\n",
    "\n",
    "        for block in self.encoder_blocks:\n",
    "\n",
    "            x, _ = block(x)\n",
    "\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ResNet152(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained):\n",
    "\n",
    "        super(ResNet152, self).__init__() \n",
    "\n",
    "        #self.model = pretrainedmodels.__dict__['resnet152'](pretrained='imagenet')                         \n",
    "\n",
    "        #self.model = torchvision.models.resnet152(pretrained=True)                          \n",
    "\n",
    "        class MyResNet18(nn.Module):\n",
    "\n",
    "            def __init__(self, resnet, resnet2):\n",
    "\n",
    "                super().__init__()\n",
    "\n",
    "                self.features = nn.Sequential(\n",
    "\n",
    "                    resnet.conv1,\n",
    "\n",
    "                    resnet.bn1,\n",
    "\n",
    "                    resnet.relu,\n",
    "\n",
    "                    resnet.maxpool,\n",
    "\n",
    "                    resnet.layer1,\n",
    "\n",
    "                    resnet.layer2,\n",
    "\n",
    "                    resnet.layer3,\n",
    "\n",
    "                    resnet.layer4\n",
    "\n",
    "                ) \n",
    "\n",
    "                self.avgpool = resnet.avgpool\n",
    "\n",
    "                self.fc = resnet.fc\n",
    "\n",
    "\n",
    "\n",
    "                self.features2 = nn.Sequential(\n",
    "\n",
    "                    resnet2.conv1,\n",
    "\n",
    "                    resnet2.bn1,\n",
    "\n",
    "                    resnet2.relu,\n",
    "\n",
    "                    resnet2.maxpool,\n",
    "\n",
    "                    resnet2.layer1,\n",
    "\n",
    "                    resnet2.layer2,\n",
    "\n",
    "                    resnet2.layer3,\n",
    "\n",
    "                    resnet2.layer4\n",
    "\n",
    "                )\n",
    "\n",
    "                self.avgpool2 = resnet2.avgpool\n",
    "\n",
    "                self.fc2 = resnet2.fc\n",
    "\n",
    "\n",
    "\n",
    "            def _forward_impl(self, x: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "                x = self.features(x)\n",
    "\n",
    "                x = self.avgpool(x)\n",
    "\n",
    "                x = torch.flatten(x, 1)\n",
    "\n",
    "                x = self.fc(x)\n",
    "\n",
    "                x2 = self.features2(x2)\n",
    "\n",
    "                x2 = self.avgpool2(x2)\n",
    "\n",
    "                x2 = torch.flatten(x2, 1)\n",
    "\n",
    "                x2 = self.fc2(x2) \n",
    "\n",
    "                return x, x2\n",
    "\n",
    "\n",
    "\n",
    "            def forward(self, x: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "                return self._forward_impl(x, x2) \n",
    "\n",
    "\n",
    "\n",
    "        model = resnet152(weights=\"DEFAULT\")\n",
    "\n",
    "        model2 = resnet152(weights=\"DEFAULT\")\n",
    "\n",
    "        self.model = MyResNet18(model, model2)\n",
    "\n",
    "        self.l0 = nn.Linear(4480, 7)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        x1,x2,x3 = X.get()\n",
    "        \n",
    "        batch, _, _, _ = x1.shape        \n",
    "\n",
    "        CHANNELS = 12\n",
    "\n",
    "        model = CoreUnet(\n",
    "\n",
    "            input_dim=CHANNELS,\n",
    "\n",
    "            output_dim=1,\n",
    "\n",
    "        ).to(device)   \n",
    "\n",
    "        x2 = self.model.features2(x2)\n",
    "        x3 = model(x3) \n",
    "\n",
    "        x2 = F.adaptive_avg_pool2d(x2, 1).reshape(batch, -1) \n",
    "        x3 = F.adaptive_avg_pool2d(x3, 1).reshape(batch, -1) \n",
    "\n",
    "        if torch.any(x1 != torch.zeros_like(x1)):        \n",
    "            x1 = self.model.features(x1)\n",
    "            x1 = F.adaptive_avg_pool2d(x1, 1).reshape(batch, -1)   \n",
    "            x = torch.cat((x1, x2, x3), 1)\n",
    "        else:\n",
    "            x = torch.cat((x2,x3),1)\n",
    "\n",
    "        l0 = self.l0(x)\n",
    "\n",
    "        return l0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dataloader instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(base_path + \"test/test-set.csv\")\n",
    "train_df = pd.read_csv(base_path + \"train/train-set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataloader instantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MYCDataset(train_data_names)\n",
    "val_set = MYCDataset(val_data_names)\n",
    "test_set  = MYCDataset(test_data_names,train=False)\n",
    "\n",
    "train_set = as_classification_dataset(train_set)\n",
    "val_set = as_classification_dataset(val_set)\n",
    "\n",
    "benchmark = nc_benchmark(train_set,val_set,n_experiences=7,task_labels=True,shuffle=True,seed=SEED)\n",
    "    \n",
    "# train_dataloader = DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "# val_dataloader = DataLoader(val_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "# test_dataloader = DataLoader(test_set,batch_size=BATCH_SIZE*2,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mResNet152\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = ResNet152(pretrained=True).to(device) \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    loggers=[interactive_logger, tb_logger]\n",
    ")\n",
    "\n",
    "\n",
    "cl_strategy = Replay(\n",
    "    model, optimizer,criterion,\n",
    "    train_mb_size=32,\n",
    "    train_epochs=5,\n",
    "    eval_mb_size=32,\n",
    "    device=device,\n",
    "    evaluator=eval_plugin\n",
    ")\n",
    "\n",
    "\n",
    "print(benchmark)\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    res = cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
