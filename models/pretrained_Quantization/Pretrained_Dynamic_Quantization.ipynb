{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earth Observation - Land Classification on EuroSAT dataset\n",
    "\n",
    "* EuroSAT is a large-scale land use and land cover classification dataset derived from multispectral Sentinel-2 satellite imagery covering European continent. \n",
    "\n",
    "* EuroSAT is composed of 27,000 georeferenced image patches (64 x 64 pixels) - each patch comprises 13 spectral bands (optical through to shortwave infrared ) resampled to 10m spatila resolution and labelled with one of 10 distinct land cover classes: AnnualCrop, Forest, HerbaceousVegetation, Highway, Industrial, Pasture, PermanentCrop, Residential, River, SeaLake. \n",
    "\n",
    "* Full details including links to journal papers and download instructions may be found here: https://github.com/phelber/eurosat.\n",
    "\n",
    "![alt text](https://github.com/phelber/EuroSAT/blob/master/eurosat_overview_small.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Setup Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "from time import time_ns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import EuroSAT\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingWarmRestarts\n",
    "from avalanche.benchmarks.datasets import default_dataset_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configure settings for the corresponding optimizers and schedulers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model'\n",
    "quantized_model_path =\"./quantized_model\"\n",
    "use_AdamW = True # Flag utilized for switching between AdamW and SGD optimizer.\n",
    "\n",
    "if use_AdamW:\n",
    "  config = {'train_test_split':0.1, # train test split ratio\n",
    "            'tr_val_split':0.2, # train validation split ratio\n",
    "            'seed':42, # random seed\n",
    "            'mini_batch_size':128,\n",
    "            'epochs':20,\n",
    "            \n",
    "            # Settings for the optimizer AdamW\n",
    "            'lr':5e-4, # learning rate\n",
    "            'weight_decay':5e-4,\n",
    "\n",
    "            # Settings for the lr_scheduler CosineAnnealingWarmRestarts\n",
    "            't_0':5, # Number of iterations for the first restart.\n",
    "            'eta_min':1e-5, # Minimum learning rate\n",
    "          }\n",
    "else:\n",
    "  config = {'train_test_split':0.1, # train test split ratio\n",
    "            'tr_val_split':0.2, # train validation split ratio\n",
    "            'seed':42, # random seed\n",
    "            'mini_batch_size':128,\n",
    "            'epochs':100,\n",
    "\n",
    "            # Settings for the optimizer SGD\n",
    "            'lr':1e-4, # learning rate\n",
    "            'weight_decay':5e-4,  \n",
    "            'momentum':0.9,\n",
    "\n",
    "            # Settings for the lr_scheduler MultiStepLR\n",
    "            'milestones':[50,75,90], # List of epoch indices.\n",
    "            'gamma':0.2, # Multiplicative factor of learning rate decay.\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define a function to set a random seed for reproducibility assurance:\n",
    "* Manage sources of randomness that may lead to varying behaviors in multiple executions of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed) # set python seed\n",
    "    np.random.seed(seed) # seed the global NumPy random number generator(RNG)\n",
    "    torch.manual_seed(seed) # seed the RNG for all devices(both CPU and CUDA) \n",
    "\n",
    "set_random_seed(seed=config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tracks the active GPU and allocates all new CUDA tensors on that device by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform pipeline\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Dataset download\n",
    "ssl._create_default_https_context = ssl._create_unverified_context # Turn off the ssl verification.\n",
    "dataset_path = default_dataset_location(\"eurosat\")\n",
    "dataset = EuroSAT(root=dataset_path, transform=transform, download=True)\n",
    "\n",
    "# Train/Test split\n",
    "n = int(len(dataset) * (1 - config['train_test_split']))\n",
    "eurosat_train, eurosat_test = torch.utils.data.random_split(dataset, [n, len(dataset) - n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select an image from the Training Data for visualization and print its corresponding label.\n",
    "classes = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
    "random_index = random.randrange(len(eurosat_train)) # random select an index\n",
    "random_image = eurosat_train[random_index][0].numpy().transpose((1, 2, 0))\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "random_image = std * random_image + mean\n",
    "random_image = np.clip(random_image, 0, 1)\n",
    "print(\"Image\", random_index, \"'s label :\", eurosat_train[random_index][1], classes[eurosat_train[random_index][1]])\n",
    "plt.imshow(random_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Instantiate PyTorch data loader that feeds the image tensors to our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tr/Val split\n",
    "n = int(len(eurosat_train) * config['tr_val_split'])\n",
    "eurosat_tr, eurosat_val = torch.utils.data.random_split(eurosat_train, [n, len(eurosat_train) - n])\n",
    "\n",
    "# instantiate data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(eurosat_tr, batch_size=config['mini_batch_size'], shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(eurosat_val, batch_size=config['mini_batch_size'], shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(eurosat_test, batch_size=config['mini_batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create a ResNet-50 model using torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "model = models.quantization.resnet50(weights='DEFAULT')\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define criterion, optimizer, and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Experiment with various optimizers and schedulers.\n",
    "if use_AdamW: \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=config['t_0'], eta_min=config['eta_min'])\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "    scheduler = MultiStepLR(optimizer, milestones=config['milestones'], gamma=config['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the Training Loop\n",
    "Below, we have a function that performs one training epoch. It enumerates data from the DataLoader, and on each pass of the loop does the following:\n",
    "\n",
    "* Gets a batch of training data from the DataLoader\n",
    "\n",
    "* Zeros the optimizerâ€™s gradients\n",
    "\n",
    "* Performs an inference - that is, gets predictions from the model for an input batch\n",
    "\n",
    "* Calculates the loss for that set of predictions vs. the labels on the dataset\n",
    "\n",
    "* Calculates the backward gradients over the learning weights\n",
    "\n",
    "* Tells the optimizer to perform one learning step - that is, adjust the modelâ€™s learning weights based on the observed gradients for this batch, according to the optimization algorithm we chose\n",
    "\n",
    "* Finally, it reports the averaged epoch loss for comparison with a validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model):\n",
    "    running_loss = 0.\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Here, we use enumerate(train_dataloader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for _, data in enumerate(train_dataloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs.to(device))\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        _, predictions = outputs.max(dim=-1)\n",
    "        num_correct += (predictions == labels.to(device)).sum()\n",
    "        num_samples += predictions.size(0)\n",
    "        \n",
    "    train_loss = running_loss/len(train_dataloader)\n",
    "    train_acc = float(num_correct)/float(num_samples)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Evaluate the Trained Model on Validation Data\n",
    "* Perform validation by examining the relative loss on a dataset that was not utilized for training.\n",
    "\n",
    "* Save a copy of the model\n",
    "\n",
    "* Utilize TensorBoard for reporting. (This necessitates launching TensorBoard from the command line and accessing it in a separate browser tab.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "def train(model):\n",
    "        # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('./eurosat_trainer_{}'.format(timestamp))\n",
    "    epoch_number = 0\n",
    "    tr_acc = 0.0\n",
    "    best_vloss = 1_000_000.\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        print('============= EPOCH {} ============='.format(epoch_number + 1))\n",
    "    \n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss, tr_acc = train_one_epoch(model)\n",
    "    \n",
    "        running_vloss = 0.0\n",
    "        val_acc = 0.0\n",
    "        num_vcorrect = 0\n",
    "        num_vsamples = 0\n",
    "    \n",
    "        # Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization.\n",
    "        model.eval()\n",
    "    \n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_dataloader):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model(vinputs.to(device))\n",
    "                vloss = criterion(voutputs, vlabels.to(device))\n",
    "                running_vloss += vloss\n",
    "                _, vpredictions = voutputs.max(dim=-1)\n",
    "                num_vcorrect += (vpredictions == vlabels.to(device)).sum()\n",
    "                num_vsamples += vpredictions.size(0)\n",
    "    \n",
    "        avg_vloss = running_vloss / len(val_dataloader)\n",
    "        val_acc = float(num_vcorrect)/float(num_vsamples)\n",
    "        print('LOSS : train {} | valid {}'.format(round(avg_loss, 4), round(avg_vloss.item(), 4)))\n",
    "        print('ACC  : train {}% | valid {}%'.format(round(tr_acc*100, 2), round(val_acc*100),2))\n",
    "    \n",
    "        # Log the running loss averaged per epoch for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                        epoch_number + 1)\n",
    "        writer.add_scalars('Training vs. Validation Accuracy',\n",
    "                        { 'Training' : tr_acc, 'Validation' : val_acc },\n",
    "                        epoch_number + 1)\n",
    "        writer.flush()\n",
    "    \n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "        epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model,dataloader):\n",
    "    start=time_ns()\n",
    "    running_tloss = 0.\n",
    "    test_acc = 0.\n",
    "    num_tcorrect = 0\n",
    "    num_tsamples = 0\n",
    "    with torch.no_grad():\n",
    "        for _, tdata in enumerate(dataloader):\n",
    "            tinputs, tlabels = tdata\n",
    "            toutputs = model(tinputs.to(device))\n",
    "            tloss = criterion(toutputs, tlabels.to(device))\n",
    "            running_tloss += tloss\n",
    "            _, tpredictions = toutputs.max(dim=-1)\n",
    "            num_tcorrect += (tpredictions == tlabels.to(device)).sum()\n",
    "            num_tsamples += tpredictions.size(0)\n",
    "    avg_tloss = running_tloss/len(dataloader)\n",
    "    test_acc = float(num_tcorrect)/float(num_tsamples)\n",
    "    end=time_ns()\n",
    "    return avg_tloss,test_acc, end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model)\n",
    "train(model)\n",
    "\n",
    "model.eval()\n",
    "model.qconfig = torch.quantization.get_default_qconfig('x86')\n",
    "\n",
    "fused_model = torch.quantization.fuse_modules(model,[['conv1', 'bn1', 'relu']])\n",
    "\n",
    "prepared_model = torch.quantization.prepare(fused_model)\n",
    "\n",
    "eval_loop(prepared_model,val_dataloader)\n",
    "\n",
    "prepared_model.eval()\n",
    "quantized_model = torch.quantization.convert(prepared_model)\n",
    "\n",
    "#quantized_model = torch.quantization.quantize_dynamic(model)#,mapping=parameters_to_be_quantized)\n",
    "torch.jit.save(torch.jit.script(quantized_model),quantized_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Models size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model_path, label=\"\"):\n",
    "    size=os.path.getsize(model_path)\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    return size\n",
    "\n",
    "f=print_size_of_model(model_path,\"fp32\")\n",
    "q=print_size_of_model(quantized_model_path,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Models evaluation performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Evaluate the models on the Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.quantization.resnet50()\n",
    "#model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "#model.state_dict = torch.load(model_path)\n",
    "#model.to(device)\n",
    "#model.eval()\n",
    "#\n",
    "#quantized_model = models.quantization.resnet50()\n",
    "#quantized_model.fc = nn.Linear(quantized_model.fc.in_features, num_classes)\n",
    "#quantized_model = torch.quantization.fuse_modules(quantized_model,[['conv1', 'bn1', 'relu']])\n",
    "#torch.jit.load(quantized_model_path)\n",
    "#quantized_model.eval()\n",
    "\n",
    "model = models.quantization.resnet50()\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.state_dict = torch.load(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_avg_tloss, model_test_acc, model_time = eval_loop(model,test_dataloader)\n",
    "print(f\"---> Model <---\")\n",
    "print(f'Loss: {round(model_avg_tloss.item(),4)}')\n",
    "print(f'Accuracy: {round(model_test_acc*100,2)}')\n",
    "print(f\"Time (ns): {model_time}\")\n",
    "\n",
    "quantized_model_avg_tloss, quantized_model_test_acc, quantized_model_time = eval_loop(quantized_model,test_dataloader) \n",
    "print(f\"---> Quantized Model <---\")\n",
    "print(f'Loss: {round(quantized_model_avg_tloss.item(),4)}')\n",
    "print(f'Accuracy: {round(quantized_model_test_acc*100,2)}')\n",
    "print(f\"Time (ns): {quantized_model_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
