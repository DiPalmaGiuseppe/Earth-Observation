{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earth Observation - Land Classification on EuroSAT dataset\n",
    "\n",
    "* EuroSAT is a large-scale land use and land cover classification dataset derived from multispectral Sentinel-2 satellite imagery covering European continent. \n",
    "\n",
    "* EuroSAT is composed of 27,000 georeferenced image patches (64 x 64 pixels) - each patch comprises 13 spectral bands (optical through to shortwave infrared ) resampled to 10m spatila resolution and labelled with one of 10 distinct land cover classes: AnnualCrop, Forest, HerbaceousVegetation, Highway, Industrial, Pasture, PermanentCrop, Residential, River, SeaLake. \n",
    "\n",
    "* Full details including links to journal papers and download instructions may be found here: https://github.com/phelber/eurosat.\n",
    "\n",
    "![alt text](https://github.com/phelber/EuroSAT/blob/master/eurosat_overview_small.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Setup Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import EuroSAT\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configure settings for the corresponding optimizers and schedulers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model'\n",
    "use_AdamW = True # Flag utilized for switching between AdamW and SGD optimizer.\n",
    "\n",
    "if use_AdamW:\n",
    "  config = {'train_test_split':0.1, # train test split ratio\n",
    "            'tr_val_split':0.2, # train validation split ratio\n",
    "            'seed':42, # random seed\n",
    "            'mini_batch_size':128,\n",
    "            'epochs':20,\n",
    "            \n",
    "            # Settings for the optimizer AdamW\n",
    "            'lr':1e-3, # learning rate\n",
    "            'weight_decay':5e-4,\n",
    "\n",
    "            # Settings for the lr_scheduler CosineAnnealingWarmRestarts\n",
    "            't_0':5, # Number of iterations for the first restart.\n",
    "            'eta_min':1e-5, # Minimum learning rate\n",
    "          }\n",
    "else:\n",
    "  config = {'train_test_split':0.1, # train test split ratio\n",
    "            'tr_val_split':0.2, # train validation split ratio\n",
    "            'seed':42, # random seed\n",
    "            'mini_batch_size':128,\n",
    "            'epochs':100,\n",
    "\n",
    "            # Settings for the optimizer SGD\n",
    "            'lr':1e-4, # learning rate\n",
    "            'weight_decay':5e-4,  \n",
    "            'momentum':0.9,\n",
    "\n",
    "            # Settings for the lr_scheduler MultiStepLR\n",
    "            'milestones':[50,75,90], # List of epoch indices.\n",
    "            'gamma':0.2, # Multiplicative factor of learning rate decay.\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define a function to set a random seed for reproducibility assurance:\n",
    "* Manage sources of randomness that may lead to varying behaviors in multiple executions of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed) # set python seed\n",
    "    np.random.seed(seed) # seed the global NumPy random number generator(RNG)\n",
    "    torch.manual_seed(seed) # seed the RNG for all devices(both CPU and CUDA) \n",
    "\n",
    "set_random_seed(seed=config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tracks the active GPU and allocates all new CUDA tensors on that device by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform pipeline\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Dataset download\n",
    "ssl._create_default_https_context = ssl._create_unverified_context # Turn off the ssl verification.\n",
    "dataset_path = \"../../\"\n",
    "dataset = EuroSAT(root=dataset_path, transform=transform, download=True)\n",
    "\n",
    "# Train/Test split\n",
    "n = int(len(dataset) * (1 - config['train_test_split']))\n",
    "eurosat_train, eurosat_test = torch.utils.data.random_split(dataset, [n, len(dataset) - n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select an image from the Training Data for visualization and print its corresponding label.\n",
    "classes = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
    "random_index = random.randrange(len(eurosat_train)) # random select an index\n",
    "random_image = eurosat_train[random_index][0].numpy().transpose((1, 2, 0))\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "random_image = std * random_image + mean\n",
    "random_image = np.clip(random_image, 0, 1)\n",
    "print(\"Image\", random_index, \"'s label :\", eurosat_train[random_index][1], classes[eurosat_train[random_index][1]])\n",
    "plt.imshow(random_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Instantiate PyTorch data loader that feeds the image tensors to our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tr/Val split\n",
    "n = int(len(eurosat_train) * config['tr_val_split'])\n",
    "eurosat_tr, eurosat_val = torch.utils.data.random_split(eurosat_train, [n, len(eurosat_train) - n])\n",
    "\n",
    "# instantiate data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(eurosat_tr, batch_size=config['mini_batch_size'], shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(eurosat_val, batch_size=config['mini_batch_size'], shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(eurosat_test, batch_size=config['mini_batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create a ResNet-50 model using torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "model = models.resnet50()\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define criterion, optimizer, and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Experiment with various optimizers and schedulers.\n",
    "if use_AdamW: \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=config['t_0'], eta_min=config['eta_min'])\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "    scheduler = MultiStepLR(optimizer, milestones=config['milestones'], gamma=config['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the Training Loop\n",
    "Below, we have a function that performs one training epoch. It enumerates data from the DataLoader, and on each pass of the loop does the following:\n",
    "\n",
    "* Gets a batch of training data from the DataLoader\n",
    "\n",
    "* Zeros the optimizer’s gradients\n",
    "\n",
    "* Performs an inference - that is, gets predictions from the model for an input batch\n",
    "\n",
    "* Calculates the loss for that set of predictions vs. the labels on the dataset\n",
    "\n",
    "* Calculates the backward gradients over the learning weights\n",
    "\n",
    "* Tells the optimizer to perform one learning step - that is, adjust the model’s learning weights based on the observed gradients for this batch, according to the optimization algorithm we chose\n",
    "\n",
    "* Finally, it reports the averaged epoch loss for comparison with a validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Here, we use enumerate(train_dataloader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for _, data in enumerate(train_dataloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs.to(device))\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        _, predictions = outputs.max(dim=-1)\n",
    "        num_correct += (predictions == labels.to(device)).sum()\n",
    "        num_samples += predictions.size(0)\n",
    "        \n",
    "    train_loss = running_loss/len(train_dataloader)\n",
    "    train_acc = float(num_correct)/float(num_samples)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Evaluate the Trained Model on Validation Data\n",
    "* Perform validation by examining the relative loss on a dataset that was not utilized for training.\n",
    "\n",
    "* Save a copy of the model\n",
    "\n",
    "* Utilize TensorBoard for reporting. (This necessitates launching TensorBoard from the command line and accessing it in a separate browser tab.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "\n",
    "directory_path=\"./model\"\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('./eurosat_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "tr_acc = 0.0\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print('============= EPOCH {} ============='.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, tr_acc = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    val_acc = 0.0\n",
    "    num_vcorrect = 0\n",
    "    num_vsamples = 0\n",
    "\n",
    "    # Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs.to(device))\n",
    "            vloss = criterion(voutputs, vlabels.to(device))\n",
    "            running_vloss += vloss\n",
    "            _, vpredictions = voutputs.max(dim=-1)\n",
    "            num_vcorrect += (vpredictions == vlabels.to(device)).sum()\n",
    "            num_vsamples += vpredictions.size(0)\n",
    "\n",
    "    avg_vloss = running_vloss / len(val_dataloader)\n",
    "    val_acc = float(num_vcorrect)/float(num_vsamples)\n",
    "    print('LOSS : train {} | valid {}'.format(round(avg_loss, 4), round(avg_vloss.item(), 4)))\n",
    "    print('ACC  : train {}% | valid {}%'.format(round(tr_acc*100, 2), round(val_acc*100),2))\n",
    "\n",
    "    # Log the running loss averaged per epoch for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation Accuracy',\n",
    "                    { 'Training' : tr_acc, 'Validation' : val_acc },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model evaluation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load the model with the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50()\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate the top-performing model on the Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_tloss = 0.\n",
    "test_acc = 0.\n",
    "num_tcorrect = 0\n",
    "num_tsamples = 0\n",
    "with torch.no_grad():\n",
    "    for _, tdata in enumerate(test_dataloader):\n",
    "        tinputs, tlabels = tdata\n",
    "        toutputs = model(tinputs.to(device))\n",
    "        tloss = criterion(toutputs, tlabels.to(device))\n",
    "        running_tloss += tloss\n",
    "        _, tpredictions = toutputs.max(dim=-1)\n",
    "        num_tcorrect += (tpredictions == tlabels.to(device)).sum()\n",
    "        num_tsamples += tpredictions.size(0)\n",
    "\n",
    "avg_tloss = running_tloss/len(test_dataloader)\n",
    "test_acc = float(num_tcorrect)/float(num_tsamples)\n",
    "print(f'Loss: {round(avg_tloss.item(),4)}')\n",
    "print(f'Accuracy: {round(test_acc*100,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Predict the label of an image from the Test Data and visualize the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random select an index\n",
    "random_index = random.randrange(len(tinputs)) # \n",
    "\n",
    "# Transform the image from the final batch of Test Data using the chosen index.\n",
    "random_image = tinputs[random_index].numpy().transpose((1, 2, 0))\n",
    "random_image = std * random_image + mean\n",
    "random_image = np.clip(random_image, 0, 1)\n",
    "\n",
    "# Display the image along with its predicted and actual labels.\n",
    "print(\"Image\", random_index, \"'s label :\", tlabels[random_index].item(), classes[tlabels[random_index].item()], \"| Predicted label :\", tpredictions[random_index].detach().cpu().item(), classes[tpredictions[random_index].detach().cpu().item()])\n",
    "plt.imshow(random_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
